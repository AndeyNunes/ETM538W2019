---
title: "ETM 538, Winter '19, Project"
author: "Jordan Hilton, Andey Nunes, Mengyu Li, Peter Boss"
date: "March 13, 2019"
output: pdf_document
subtitle: Predicting Blood Donations
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# BEFORE KNITTING THIS DOCUMENT, ENSURE THAT THE FOLLOWING PACKAGES
# ARE INSTALLED & UPDATED 

packages <- c("caret", "class", "corrplot", "e1071", "knitr", "Metrics", "pander", "ranger", "rpart", "rpart.plot", "rsample", "tidyverse")

lapply(packages, require, character.only = T)

options(digits = 4, scipen = 999)

set.seed(123)
```


# Introduction  

We are analyzing a data set on blood donations, obtained from a recent Driven Data competition: https://www.drivendata.org/competitions/2/warm-up-predict-blood-donations/. The goal of the competition is to use data on past donations to predict whether a donation was made by a specific patient in the month of March 2007.  

If our model successfully predicts donations, we can meet the business goal of creating accurate forecasting for blood donations. While the data in this specific instance comes from a blood donation truck in Taiwan, having a reliable predictive model would help blood donations services, hospitals, and the healthcare system at large---they could plan for storage and transportation of blood products, determine appropriate staffing levels, have more reliable schedules for procedures that require blood, etc. Having a more robust understanding of blood donation practices between countries could also help governments determine and employ best practices for encouraging regular and predictable blood donation from more people.  

Our analysis uses 4 models:
\begin{enumerate}
\item Naive linear regression  
\item K Nearest Neighbors  
\item Decision Trees  
\item Cross-validation with random forest, which includes another linear regression  
\end{enumerate}

Using lowest error rate in the predictions as the criterion, we determined that the strongest model is the KNN, with error rates as follows:

KNN Error: 10.5% at k=3

Decision Tree Error: 28.1%

Cross-Validation Logistic Regression Error: 23.8%

Cross-Validation Random Forest: 21.7%

### Initial Data Exploration  

```{r load data}
training <- read_csv("projectdata.csv")
glimpse(training)
```

The data set comprises 576 observations on each of the variables below. To facilitate easier analysis, we have renamed the variables.  

+ A count variable, functioning as a unique ID for each person who donated blood. This was not a meaningful component of our analysis, so we ignored it in our models.
+ `Months since Last Donation` indicates how many months since the most recent donation event. Renamed as `recency`.
+ `Months since First Donation` indicates the total time span in months since the first donation event. Renamed as `time`.
+ `Number of Donations`. Renamed as `freq`, for "frequency."
+ `Total Volume Donated (c.c.)`. The original data set makes note that this field indicates the monetary measure being used for the business case. Renamed as `vol`, for "volume."
+ `Made Donation in March 2007`. The binary variable that we are trying to classify and predict. Renamed as `target`.  

```{r correlations}
# remove ID variable
training <- training[,-1]

# create and apply a vector of names
names(training) <- c("recency", "freq", "vol", "time", "target")
training_mat <- data.matrix(training, rownames.force = T)

```


We examine a plot of the correlations between the variables:  
```{r, out.width="80%", echo=FALSE}
corr_mat <- cor(training_mat)
corrplot.mixed(corr_mat)

```

We see there is a perfect correlation between `freq` and `vol`. We note that the `vol` values are exactly 250 times the `freq` values, implying that people are donating exactly 250 CCs at a time. Since `vol` is an exact linear multiple of another variable, we do not include it in our analyses. 


-------

-------

-------


### Model 1: Naive Linear Regression  

Our first model is ordinary linear regression. We chose this model because it's a typical starting point for data analysis, and it's easy to run and interpret. As discussed below, the results did not prove to be useful, but we still consider it a valuable starting point.  

<!--
First we load and inspect our data.  

```{r load linear model}
data <- read.csv("projectdata.csv")
data <- data[,-1]
names(data)
names(data) <- c("recency", "freq", "vol", "time", "target")
head(data)
```

As noted above, `vol` is a linear multiple of `freq`.  Including both would cause problems in a linear model because one is a linear multiple of the other, so `vol` was not included here.  

```{r dropandcale, echo=FALSE}
#data<-data[-c(1,4)]
```

-->

We create and inspect the linear model.  

```{r linearmodel}
linearmodel <- lm(target ~ recency + freq + time, data=training)
summary(linearmodel)
```


While the model as a whole is statistically significant with a p-value of $2.6 \cdot 10^{-15}$, the low $R^2$ indicates that our 3 independent variables do a poor job predicting blood donation in the linear model. Each variable is significant in the full linear model, but we formally check that it's appropriate to use each variable.  

```{r lm step}
step(linearmodel)
```

Each variable does contribute sufficiently to a reduction in the sum of the squares of error, and we can't reduce our AIC by eliminating a variable. We examine the standard residual plots to verify the errors are normally distributed.  

```{r lm plot, out.width="50%"}
plot(linearmodel)
```

The residual plots all look awful. Most notably, each plot splits the errors into two distinct groups. The most basic checks for normality are violated in each graph, and there are high leverage points. We conclude that the data does not follow a normal distribution, so linear regression is inappropriate for this data set. Much of the difficulty with this analysis probably comes from the fact that the response variable is binary. If we were continuing to apply regression, logistic regression would probably be appropriate here. Instead, we chose types of analysis that were more in line with the material learned in this class.  

Note: This model does not include interactions between the input variables, but we also tried a linear regression model including those interactions. The $R^2$ value was slightly higher but still far too low to be useful (0.16 rather than 0.11), and the errors also exhibited the same problem of lack of normality. We conclude that including variable interactions did not help make linear regression appropriate for this data. The analysis was very similar to what has already been presented, so it is omitted here.  

```{r lm_int, include=FALSE}
# a version of the linear model that includes interaction between the input variables
# there was no significant difference between the models, so this one is omitted

linearmodel_int <- lm(target ~ recency * freq * time - recency:time, data=training)
summary(linearmodel_int)

step(linearmodel_int)
plot(linearmodel_int)
```



-------

### Model 2: K Nearest Neighbors  

```{r knn load, echo=FALSE}
data <- read.csv("projectdata.csv")
instances <- data[-c(1,4,6)] ##drop the index column and the result column for working purposes
```

We begin our Nearest Neighbor analysis by scaling each column by the maximum value. Every entry in the columns is now between 0 and 1. Next we assign weights to each variable.  

```{r knn assignweights}
weights <- c(1/74,1/50,1/12500,1/98) ## make a vector of independent variable weights, scaling each by the max value to start
names(weights) <- c("monthssincelast", "numberdonations", "monthssincefirst")  ##name them so we don't forget
scaledinstances <- cbind(instances[,1]*weights[1],instances[,2]*weights[2],instances[,3]*weights[3])
## make a scaled version of our instances so that the distances are equivalent
```

We divide our working data into two buckets: one for training, and one for calculating nearest neighbors to assign weights.

```{r knn bucketize}
validationbucket <- scaledinstances[501:576,]
databucket <- scaledinstances[1:500,]
```

We write a function to calculate distances between two different points. We choose the Euclidean distance for two reasons. One, some of our data is in numeric form instead of factors, so Euclidean distance is more appropriate for those values. Two, we will compare our results against a prebuilt nearest-neighbors package, and that package is keyed to Euclidean distance.  

```{r knn calculatedistance}
distance <- function(x,y){
  result <- dist(rbind(x,y), method="euclidean") ## find the manhattan distance between two instances
  return(as.numeric(result)) #the result is weirdly vectorized so we just grab the value out of it
}
```

Here is an example of the distance calculation, working on one of our validation points and one of our data points.  

```{r knn example1}
databucket[1,]
validationbucket[1,]
distance(databucket[1,], validationbucket[1,])
```

We create a table to calculate the distances between each of our validation points and each of the training points. Each column, iterated on $j$, is a validation point. Each row, iterated on $i$, is a data point.  

```{r knn distancestable}
distancesbyrow <- data.frame() #length(data) rows iterated by i for data, length(validation) columns iterated by j for validation

for(i in 1:length(databucket[,1])){
  for(j in 1:length(validationbucket[,1])){
    distancesbyrow[i,j]<-distance(databucket[i,],validationbucket[j,])  # calculate the distance between every  possible row and data row, store the results
  }
}

head(distancesbyrow[,1:6])
```

We create a table for the results: which point in the data is nearest the $i$th validation point, and what is the value of that point for our `target` variable (whether or not a donation was made). We were concerned about possible cases where multiple points were nearest but that recommended different `target` outcomes.  But no such "ambiguous" values actually appeared in the analysis.  

```{r knn resultstable}
distanceresults <- data.frame() #length(validation) rows, one for each test case. first column is minimum distances, second column is occurences of the minimum distance

for(i in 1:length(validationbucket[,1])){ #traversing across our validation data
  distanceresults[i,1]<-min(distancesbyrow[,i]) #the minimum distance for each column of distancesbyrow
  distanceresults[i,2]<-sum(distanceresults[i,1]==distancesbyrow[,i]) #the number of occurences of this minimum value
  distanceresults[i,3]<-data[which.min(distancesbyrow[,i]),6] ## pulls the "donation" value for the first index which has the minimum value
  for(j in 1:length(validationbucket[,1])){ ## traversing across the distances for one possible case
    if(distancesbyrow[j,i]==distanceresults[i,1] & data[j,6]!=distanceresults[i,3]){
     distanceresults[i,3]<-"amb"
    } ## if the distance for this case is minimum AND the playvalue is not equal to the first playvalue stored, rewrite the playvalue to be ambiguous
  }
}

colnames(distanceresults)<-c("minimumdistance","occurrences", "donation")
head(distanceresults)
sum(distanceresults$occurrences==1) # the number of possible classes that have a unique closest neighbor in the data
sum(distanceresults$donation=="amb") # the number of unambiguous results
```

We compare the results of our training against the real donation values, and we calculate the error rate.  

```{r knn errorrate}
predictedresults<-distanceresults[,3]
originaldata <- data[501:576,6]
correctanswers <- sum(predictedresults==originaldata)
errorrate_knn <- 1 - correctanswers/length(originaldata)
errorrate_knn
```

The error rate is `r round(errorrate_knn * 100, 1)`%.  

Ordinarily, our next step would be to adjust the weight vector and re-run the analysis, with a goal of minimizing the error rate. In this instance, though, there are robust existing R packages that we can use to compare results. We employ the `class` package to see another approach to the problem.  

```{r knn usinglibrary}
train<-databucket
test<-validationbucket
cl<-data[1:500,6]
libraryresults<-knn(train, test, cl, k=3, prob=TRUE)
libraryresults
```

We've chose $k = 3$ nearest neighbors, and the library returns (in the first result) the winning classification for each test row, and also the probability of that classification based on the nearest neighbors. We calculate the error rate:  

```{r knn libraryerror}
predictedresults<-as.numeric(libraryresults)-1
originaldata<-data[501:576,6]
correctanswers<-sum(predictedresults==originaldata)
errorrate<-1-correctanswers/length(originaldata)
errorrate
```

Using the same code, we checked the error rate considering the 1, 2, 3, 4 nearest neighbors:

```{r knn whichk}
k<-c(1,2,3,4)
error<-c("18.4%", "13.1%", "10.5%", "9.2%" )
pander(cbind(k,error))
```

We want to avoid overfitting, so we choose $k = 3$ nearest neighbors for the analysis. We note that the built-in package has slightly smaller error than our manual work even at $k=1$, and that at $k=3$ the error rate is half of what it was in our original work. Note also that the built-in package decides points with tied votes in their nearest neighbors at random, so you might get slightly different results when rerunning this code.

The data contest (from Data Driven) includes a specific set of test data. Here are our model's predictions against those data points.  

```{r knn predictions}
testdata<-read.csv("project test data.csv")
testdata<-testdata[-c(1,4,6)]
testdata<-cbind(testdata[,1]*weights[1],testdata[,2]*weights[2],testdata[,3]*weights[3])
train<-scaledinstances
test<-testdata
cl<-data[,6]
libraryfullresults<-knn(train, test, cl, k=, prob=TRUE)
head(libraryfullresults)
```


-------

### Model 3: Decision Tree
In this section, we first check the strcution of data frame which consist of 576 observation of 6 variables. 

```{r}
dfRawData <- read.csv('projectdata.csv')
str(dfRawData)
```

## Pre-process data
In this section, data is cleared and prepared ready for processing.

```{r}
dfModelData <- dfRawData[-1] # Exclude the first column
dfModelData <- na.omit(dfModelData) # remove any NAs
colnames(dfModelData) <- c('MonthsSinceLastDonation', 'NumberOfDonations', 'TotalVolumeDonatedCC', 'MonthsSinceFirestDonation', 'MadeDonationInMarch2007')
dfModelData$MadeDonationInMarch2007 <- as.factor(dfModelData$MadeDonationInMarch2007)
```

## Split data into train and test data
In this section, we create the train data and test date. By using p=0.8, it means the data split should be done in 80:20 ratio. And before training decision tree classifier, set.seed().

```{r}
# Split the data into training and test set
set.seed(123)
# 80% of data are selected as train data
trnSamples <- dfModelData$MadeDonationInMarch2007 %>% 
  createDataPartition(p = 0.8, list = FALSE)
trnData <- dfModelData[trnSamples, ]
testData <- dfModelData[-trnSamples, ]
```

##Trained decision tree classifier result.
We check the result of our train()method by a print dtree_fit variable. It's showing us the accuracy metrics or different values of cp as the following.
We also used cross validation to divide it into 10 folds.

```{r}
control <- trainControl(method = 'repeatedcv', number = 10, repeats = 5)
# Train the model
model_DT <- train(MadeDonationInMarch2007 ~., data = trnData, method = 'rpart',  parms = list(split = "information"), tuneLength = 10) #trcontrol = control,
# Estimate variable importantce
model_DT
```

From above, the cp= 0.04337671. We are ready to predict classes for our test set, and use predict()method.

```{r}
prp(model_DT$finalModel, box.palette = "Reds", tweak = 1.2)
```

##Confusion Matrix
The following result shows that the classifier with thre criterion as information gain is giving 0.7193 of accuracy for the test set.
The prediction accuracy of 0.7193, which means that the percentage that the actual/prefernce result is 0, and the prediction result is 0;  the actual is 1, and that prediction result is 1 is 0.7193. The error rate = 1- accuracy = 0.2807, kappa =0.1817, which means the kappa coefficient is slight.

```{r}
test_pred <- predict(model_DT, newdata = testData)
confusionMatrix(test_pred, testData$MadeDonationInMarch2007 )  #check accuracy
```



-------

# Model 4: Cross Validation Folds & Splits  

The workflow and code used here is adapted from the *Machine Learning in the Tidyverse* course by Dmitriy Gorenshteyn (DataCamp 2019). The approach involves using list-columns to store list objects (such as model output) in a tibble, which is a special R data frame. These lists can then be iterated over with specialized `map` functions from the `purrr` package to calculate and extract information.

Using the training data, we will create a 5-fold cross-validation split tibble. This will set us up to itertively generate multiple models on our new train/validation subsets.

```{r create vfold}
# using the rsample library
cv_split <- vfold_cv(training, v = 5)
print(cv_split)# uncomment the front of this line if you want to preview
```

The `cv_split` object has five rows and two columns. The first column, `splits`, is a list column containing the training and validation data. The second column is a character vector containing the fold id generated by the `vfold_cv()` function. We can iterate over the `splits` column and extract the train and validation columns 

```{r make train & validation}
cv_data <- cv_split %>%
   mutate(train = map(splits, ~training(.x)),
          validate = map(splits, ~testing(.x)))
glimpse(cv_data)
```

We've just created two new list columns containing the data that we can now train and validate models over. In the next two sections I will prepare and evaluate a logistic regression model and a random forests model on these cross-validation sets.

# Model preparation

### logistic regression model

```{r logistic regression model}
cv_models_glm <- cv_data %>%
   mutate(glm_model = map(train, ~glm(formula = target~., data = .x, family = "binomial"))
          )
```

### random forest

```{r random forest model}
# set forest parameter
n_trees <- 500
# Build a random forest model for each fold
cv_models_rf <- cv_data %>% 
  mutate(rf_model = map(train, ~ranger(formula = target~., data = .x,
                                    num.trees = n_trees,))
         
         )
```

Now we have two tibbles, one containing a set of logistic models and the cross-validation data, and the other containing the set of random forest models. We can now iterate over these tibbles to extract the actual values and generate prediction comparisons.



# Model evaluation

The *cross-validation model evaluation process* follows the same general set of steps iterated over each fold:
1. extract the actual target values from the validation set
2. use the models to make target predictions that will be compared to the actual target
3. for each fold, calculate the following:

+ Accuracy where 
+ Mean Absolute Error (`MAE`) where $$ MAE = \frac{\sum^n_{i=1}|Actual_i - Predicted_i|}{n} $$


4. Take the average over all MAE values to determine which model performs best on these sets of training & validation splits

In the following subsections, the eval code chunk follows a similar pattern: 

+ the *actual* and *predicted* values are extracted from the validation sets into a `cv_prep_`

Lets see how random forests compared.

### logistic regression model

```{r glm predict, warning=FALSE, }
# extract actual values
cv_prep_glm <- cv_models_glm %>%
   mutate(validate_actual = map(validate, ~.x$target),
          validate_predicted = map2(.x = glm_model, .y = validate, 
                                    ~predict(.x, .y, type = "response") > 0.5))
```


```{r glm eval}
#the function mae() is from library(Metrics)
#Calculate the mean absolute error for each validate fold
cv_eval_glm <- cv_prep_glm %>%
  mutate(validate_mae = map2_dbl(.x = validate_actual,
                                 .y = validate_predicted,
                                 ~mae(actual = .x, predicted = .y)),
         glm_accuracy = map2_dbl(.x = validate_actual,
                                 .y = validate_predicted,
                                 ~accuracy(actual = .x, predicted = .y)),
         glm_precision = map2_dbl(.x = validate_actual,
                                 .y = validate_predicted,
                                 ~precision(actual = .x, predicted = .y)),
         glm_recall = map2_dbl(.x = validate_actual,
                                 .y = validate_predicted,
                                 ~recall(actual = .x, predicted = .y)))

# Print the validate_mae column
glm_results <- tibble(foldID = 1:5,
                      MAE = cv_eval_glm$validate_mae,
                      accuracy = cv_eval_glm$glm_accuracy,
                      precision = cv_eval_glm$glm_precision,
                      recall = cv_eval_glm$glm_recall
                      )

kable(glm_results)

```

The average mean absolute error across all logistic regression models was `r mean(cv_eval_glm$validate_mae)`. Looks like logistic regression has a similar error rate as the linear regression model. Let's see how the random forest model did.


### random forest

```{r rf predict}
# Generate predictions using the random forest model
cv_prep_rf <- cv_models_rf %>% 
  mutate(validate_actual = map(validate, ~.x$target),
         validate_predicted = map2(.x = rf_model, .y = validate, ~predict(.x, .y, type = "response")$predictions > 0.5))
```


```{r rf quick eval}

# Calculate validate MAE for each fold
cv_eval_rf <- cv_prep_rf %>% 
  mutate(validate_mae = map2_dbl(validate_actual, validate_predicted, ~mae(actual = .x, predicted = .y)))

# Print the validate_mae column
cv_eval_rf$validate_mae

# Calculate the mean of validate_mae column
```

The mean absolute error rate was `r mean(cv_eval_rf$validate_mae)`. We can tune some model parameters to see if this can be improved. 

**Tune hyper-parameters**

The `mtry` parameter selects a random subset of variables to model. Since we only have 5 variables to begin with, we are somewhat limited in the range of this parameter, but lets try it anyway. We start by creating another tibble containing a set of the 5-fold cross-validation data for each `mtry` value. Here the values will range between 1 and 3 so we will get 15 sets of training|validation splits respective of the parameter setting. We will iterate over this tibble to create 15 random forest models and continue to utilize the list-column workflow to predict on validation splits and evaluate all of these models.

```{r hyper parameters}

# Prepare for tuning cross validation folds by varying mtry
cv_tune <- cv_data %>% 
   crossing(mtry = 1:3)

# Build a model for each fold & mtry combination
cv_model_tunerf <- cv_tune %>% 
  mutate(rf_model = 
            map2(.x = train, .y = mtry, 
                 ~ranger(formula = target~., data = .x, 
                         mtry = .y, num.trees = n_trees)))

glimpse(cv_model_tunerf) 
```

Now lets evaluate the models using the mean absolute error, accuracy, precision, and recall metrics that we used to evaluate the logistic regression model.

```{r rf predict and evaluate}
# Generate validate predictions for each model
cv_prep_tunerf <- cv_model_tunerf %>% 
  mutate(rf_validate_actual = map(validate, ~.x$target),
     rf_validate_predicted = map2(.x = rf_model, .y = validate, ~predict(.x, .y, type = "response")$predictions > 0.5))

# Calculate validate MAE for each fold and mtry combination
cv_eval_tunerf <- cv_prep_tunerf %>% 
  mutate(rf_validate_mae = map2_dbl(.x = rf_validate_actual, .y = rf_validate_predicted, ~mae(actual = .x, predicted = .y)),
         rf_accuracy = map2_dbl(.x = rf_validate_actual,
                                 .y = rf_validate_predicted,
                                 ~accuracy(actual = .x, predicted = .y)),
         rf_precision = map2_dbl(.x = rf_validate_actual,
                                 .y = rf_validate_predicted,
                                 ~precision(actual = .x, predicted = .y)),
         rf_recall = map2_dbl(.x = rf_validate_actual,
                                 .y = rf_validate_predicted,
                                 ~recall(actual = .x, predicted = .y)))
```


```{r summarize rf model findings}
# Calculate the mean validate_mae for each mtry used  
rf_eval_summary <- cv_eval_tunerf %>% 
   group_by(mtry) %>% 
   summarise(rf_mean_mae = mean(rf_validate_mae),
            rf_mean_accuracy = mean(rf_accuracy),
            rf_mean_precision = mean(rf_precision),
            rf_mean_recall = mean(rf_recall))
kable(rf_eval_summary)
```

It looks like we get lowest error rate using mtry 2, but best recall and accuracy on the mtry 3. Still the mean error rate for any model is much higher than other methods previously, therefore, it will not be used to generate predictions for the DrivenData contest. If we did want to generate the vector of predictions for the test data, here is how we would go about doing that in four steps:

1. Select random forest model with the lowest average mae (or other metric).

```{r best mtry}
best <- filter(cv_eval_tunerf, rf_validate_mae == min(cv_eval_tunerf$rf_validate_mae))
kable(tibble(id = best$id, mtry = best$mtry))
```

2. Use these parameters to train our best rf model.

```{r select best rf model}
# Build the model using all training data and the best performing parameter
best_model <- ranger(formula = target~., data = training,
                     mtry = best$mtry, num.trees = n_trees)
```

3. Load test data

```{r load test data}
test_data <- read_csv("project test data.csv")
names(test_data) <- c("id", "recency", "freq", "vol", "time")
# remove id
head(test_data)
test <- select(test_data,-id, -vol)
```

4. Generate prediction vector

```{r predict test data vector, eval=FALSE}
test_predicted <- predict(best_model, test)$predictions
rf_test_prediction <- cbind(test_data$id, test_predicted)

```


# Conclusion

The knn model had the best error rate.


# References

Data is courtesy of Yeh, I-Cheng via the UCI Machine Learning repository (https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center)

https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.names

Code examples were borrowed from DataCamp course material presented by Dmitriy Gorenshteyn, "Machine Learning in the Tidyverse" https://www.datacamp.com/courses/machine-learning-in-the-tidyverse

# Appendix

```{r random forest predicted test vector, include=F, eval=F}
kable(rf_test_prediction)
```











